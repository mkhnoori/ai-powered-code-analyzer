{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from git import Repo\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir test_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = \"test_repo/\"\n",
    "repo = Repo.clone_from(\"https://github.com/mkhnoori/complete-medical-chatbot.git\", to_path=repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = GenericLoader.from_filesystem(repo_path,\n",
    "glob = \"**/*\",\n",
    "suffixes = [\".py\"],\n",
    "parser = LanguageParser(language = \"python\", parser_threshold=500)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"from setuptools import setup, find_packages\\n\\nsetup(\\n    name = 'Generative AI Project',\\n    version = '0.0.0',\\n    author = 'Mohammad Khnoori',\\n    author_email = 'mkh_noori@yahoo.com',\\n    packages = find_packages(),\\n    install_requires = []\\n)\", metadata={'source': 'test_repo/setup.py', 'language': 'python'}),\n",
       " Document(page_content='from flask import Flask, render_template, jsonify, request\\nfrom src.helper import download_hugging_face_embeddings\\nfrom langchain_pinecone import PineconeVectorStore\\nfrom pinecone import Pinecone\\nfrom langchain_community.llms import CTransformers \\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.chains import RetrievalQA\\nfrom dotenv import load_dotenv\\nfrom src.prompt import *\\nimport os\\n\\napp = Flask(__name__)\\n\\nload_dotenv()\\n\\nPINECONE_API_KEY = os.environ.get(\\'PINECONE_API_KEY\\')\\nPINECONE_ENVIRONMENT = os.environ.get(\\'PINECONE_ENVIRONMENT\\')\\nOPENAI_API_KEY = os.environ.get(\\'OPENAI_API_KEY\\')\\n\\n\\nembeddings = download_hugging_face_embeddings()\\n\\n#Initializing the Pinecone\\npc = Pinecone(api_key=PINECONE_API_KEY, \\n              environment=PINECONE_ENVIRONMENT)\\n\\nindex_name=\"medical-chatbot\"\\n\\n#Loading the index\\ndocsearch = PineconeVectorStore.from_existing_index(index_name, embeddings)\\n\\n\\nPROMPT=PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\\n\\nchain_type_kwargs={\"prompt\": PROMPT}\\n\\n# Replace the CTransformers section with:\\nfrom langchain_openai import ChatOpenAI\\n\\nllm = ChatOpenAI(\\n    model_name=\"gpt-3.5-turbo\",\\n    temperature=0.7,\\n    openai_api_key=OPENAI_API_KEY\\n)\\n\\n\\nqa=RetrievalQA.from_chain_type(\\n    llm=llm, \\n    chain_type=\"stuff\", \\n    retriever=docsearch.as_retriever(search_kwargs={\\'k\\': 2}),\\n    return_source_documents=True, \\n    chain_type_kwargs=chain_type_kwargs)\\n\\n\\n\\n@app.route(\"/\")\\ndef index():\\n    return render_template(\\'chat.html\\')\\n\\n\\n\\n@app.route(\"/get\", methods=[\"GET\", \"POST\"])\\ndef chat():\\n    msg = request.form[\"msg\"]\\n    input = msg\\n    print(input)\\n    result=qa({\"query\": input})\\n    print(\"Response : \", result[\"result\"])\\n    return str(result[\"result\"])\\n\\n\\n\\nif __name__ == \\'__main__\\':\\n    app.run(host=\"0.0.0.0\", port= 8080, debug= True)', metadata={'source': 'test_repo/app.py', 'language': 'python'}),\n",
       " Document(page_content='from pathlib import Path\\nimport logging\\nimport os\\n\\nlogging.basicConfig(level=logging.INFO, format=\\'[%(asctime)s]: %(message)s:\\')\\n\\nlist_of_files = [\\n    \"src/__init__.py\",\\n    \"src/helper.py\",\\n    \"src/prompt.py\",\\n    \".env\",\\n    \"app.py\",\\n    \"setup.py\",\\n    \"research/trials.ipynb\",\\n    \"store_index.py\",\\n    \"static/.gitkeep\",\\n    \"templates/chat.html\"\\n]\\n\\nfor filepath in list_of_files:\\n    filepath = Path(filepath)\\n    filedir, filename = os.path.split(filepath)\\n\\n    if filedir != \"\":\\n        os.makedirs(filedir, exist_ok=True)\\n        logging.info(f\"Creating directory:{filedir} for the file {filename}\")\\n\\n    if (not os.path.exists(filepath)) or (os.path.getsize(filepath) == 0):\\n        with open(filepath, \"w\") as f:\\n            pass\\n            logging.info(f\"Creating empty file: {filepath}\")\\n\\n    else:\\n        logging.info(f\"{filename} is already exists\")', metadata={'source': 'test_repo/template.py', 'language': 'python'}),\n",
       " Document(page_content='from src.helper import load_data, text_split, download_hugging_face_embeddings\\nfrom pinecone import Pinecone, ServerlessSpec\\nfrom langchain_pinecone import PineconeVectorStore\\nfrom dotenv import load_dotenv\\nimport os\\n\\nload_dotenv()\\nPINECONE_API_KEY = os.environ.get(\\'PINECONE_API_KEY\\')\\nPINECONE_ENVIRONMENT = os.environ.get(\\'PINECONE_ENVIRONMENT\\')\\nos.environ[\\'PINECONE_API_KEY\\'] = PINECONE_API_KEY\\n\\nextracted_data = load_data(data=\"data/\")\\ntext_chunks = text_split(extracted_data)\\nembeddings = download_hugging_face_embeddings()\\n\\npc = Pinecone(api_key=PINECONE_API_KEY, environment=PINECONE_ENVIRONMENT)\\n\\nindex_name = \"medical-chatbot\"\\n\\npc.create_index(\\n    name=index_name,\\n    dimension=384, # Replace with your model dimensions\\n    metric=\"cosine\", # Replace with your model metric\\n    spec=ServerlessSpec(\\n        cloud=\"aws\",\\n        region=\"us-east-1\"\\n    ) \\n)\\n# embed each chunk and upsert the embeddings into Pinecone index\\ndocsearch = PineconeVectorStore.from_documents(\\n    documents=text_chunks,\\n    index_name=index_name,\\n    embedding=embeddings,\\n)', metadata={'source': 'test_repo/store_index.py', 'language': 'python'}),\n",
       " Document(page_content='', metadata={'source': 'test_repo/src/__init__.py', 'language': 'python'}),\n",
       " Document(page_content='from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\nfrom langchain_community.embeddings import HuggingFaceHubEmbeddings\\n\\n# Load data from PDF files\\ndef load_data(data):\\n    loader = DirectoryLoader(data,\\n                             glob=\"*.pdf\",\\n                             loader_cls=PyPDFLoader)\\n    documents = loader.load()\\n    return documents\\n\\n# split the data into text chunks\\ndef text_split(extracted_data):\\n    text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\\n    text_chunks=text_splitter.split_documents(extracted_data)\\n    return text_chunks\\n\\n# download embeddings from haggingface\\ndef download_hugging_face_embeddings():\\n    embeddings = HuggingFaceHubEmbeddings(\\n        repo_id=\\'sentence-transformers/all-MiniLM-L6-v2\\',\\n        task=\"feature-extraction\"\\n    )\\n    return embeddings', metadata={'source': 'test_repo/src/helper.py', 'language': 'python'}),\n",
       " Document(page_content='prompt_template = \"\"\"\\nUse the following pieces of context to answer the question at the end in a detailed and informative way. \\nIf you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nProvide a comprehensive explanation that includes:\\n- Definition of the condition/term\\n- Causes and risk factors\\n- Symptoms and signs\\n- Treatment options when applicable\\n- Prevention methods when applicable\\n\\n{context}\\n\\nQuestion: {question}\\nDetailed Medical Answer:\\n\"\"\"', metadata={'source': 'test_repo/src/prompt.py', 'language': 'python'})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"from setuptools import setup, find_packages\\n\\nsetup(\\n    name = 'Generative AI Project',\\n    version = '0.0.0',\\n    author = 'Mohammad Khnoori',\\n    author_email = 'mkh_noori@yahoo.com',\\n    packages = find_packages(),\\n    install_requires = []\\n)\", metadata={'source': 'test_repo/setup.py', 'language': 'python'})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_splitter = RecursiveCharacterTextSplitter.from_language(language= \"python\",\n",
    "chunk_size = 500,\n",
    "chunk_overlap = 20,\n",
    "length_function = len,\n",
    "add_start_index = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = documents_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"from setuptools import setup, find_packages\\n\\nsetup(\\n    name = 'Generative AI Project',\\n    version = '0.0.0',\\n    author = 'Mohammad Khnoori',\\n    author_email = 'mkh_noori@yahoo.com',\\n    packages = find_packages(),\\n    install_requires = []\\n)\", metadata={'source': 'test_repo/setup.py', 'language': 'python', 'start_index': 0}),\n",
       " Document(page_content='from flask import Flask, render_template, jsonify, request\\nfrom src.helper import download_hugging_face_embeddings\\nfrom langchain_pinecone import PineconeVectorStore\\nfrom pinecone import Pinecone\\nfrom langchain_community.llms import CTransformers \\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.chains import RetrievalQA\\nfrom dotenv import load_dotenv\\nfrom src.prompt import *\\nimport os\\n\\napp = Flask(__name__)\\n\\nload_dotenv()', metadata={'source': 'test_repo/app.py', 'language': 'python', 'start_index': 0}),\n",
       " Document(page_content='load_dotenv()\\n\\nPINECONE_API_KEY = os.environ.get(\\'PINECONE_API_KEY\\')\\nPINECONE_ENVIRONMENT = os.environ.get(\\'PINECONE_ENVIRONMENT\\')\\nOPENAI_API_KEY = os.environ.get(\\'OPENAI_API_KEY\\')\\n\\n\\nembeddings = download_hugging_face_embeddings()\\n\\n#Initializing the Pinecone\\npc = Pinecone(api_key=PINECONE_API_KEY, \\n              environment=PINECONE_ENVIRONMENT)\\n\\nindex_name=\"medical-chatbot\"\\n\\n#Loading the index\\ndocsearch = PineconeVectorStore.from_existing_index(index_name, embeddings)', metadata={'source': 'test_repo/app.py', 'language': 'python', 'start_index': 424}),\n",
       " Document(page_content='PROMPT=PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\\n\\nchain_type_kwargs={\"prompt\": PROMPT}\\n\\n# Replace the CTransformers section with:\\nfrom langchain_openai import ChatOpenAI\\n\\nllm = ChatOpenAI(\\n    model_name=\"gpt-3.5-turbo\",\\n    temperature=0.7,\\n    openai_api_key=OPENAI_API_KEY\\n)', metadata={'source': 'test_repo/app.py', 'language': 'python', 'start_index': 900}),\n",
       " Document(page_content='qa=RetrievalQA.from_chain_type(\\n    llm=llm, \\n    chain_type=\"stuff\", \\n    retriever=docsearch.as_retriever(search_kwargs={\\'k\\': 2}),\\n    return_source_documents=True, \\n    chain_type_kwargs=chain_type_kwargs)\\n\\n\\n\\n@app.route(\"/\")', metadata={'source': 'test_repo/app.py', 'language': 'python', 'start_index': 1220}),\n",
       " Document(page_content='def index():\\n    return render_template(\\'chat.html\\')\\n\\n\\n\\n@app.route(\"/get\", methods=[\"GET\", \"POST\"])\\ndef chat():\\n    msg = request.form[\"msg\"]\\n    input = msg\\n    print(input)\\n    result=qa({\"query\": input})\\n    print(\"Response : \", result[\"result\"])\\n    return str(result[\"result\"])\\n\\n\\n\\nif __name__ == \\'__main__\\':\\n    app.run(host=\"0.0.0.0\", port= 8080, debug= True)', metadata={'source': 'test_repo/app.py', 'language': 'python', 'start_index': 1448}),\n",
       " Document(page_content='from pathlib import Path\\nimport logging\\nimport os\\n\\nlogging.basicConfig(level=logging.INFO, format=\\'[%(asctime)s]: %(message)s:\\')\\n\\nlist_of_files = [\\n    \"src/__init__.py\",\\n    \"src/helper.py\",\\n    \"src/prompt.py\",\\n    \".env\",\\n    \"app.py\",\\n    \"setup.py\",\\n    \"research/trials.ipynb\",\\n    \"store_index.py\",\\n    \"static/.gitkeep\",\\n    \"templates/chat.html\"\\n]\\n\\nfor filepath in list_of_files:\\n    filepath = Path(filepath)\\n    filedir, filename = os.path.split(filepath)', metadata={'source': 'test_repo/template.py', 'language': 'python', 'start_index': 0}),\n",
       " Document(page_content='if filedir != \"\":\\n        os.makedirs(filedir, exist_ok=True)\\n        logging.info(f\"Creating directory:{filedir} for the file {filename}\")\\n\\n    if (not os.path.exists(filepath)) or (os.path.getsize(filepath) == 0):\\n        with open(filepath, \"w\") as f:\\n            pass\\n            logging.info(f\"Creating empty file: {filepath}\")\\n\\n    else:\\n        logging.info(f\"{filename} is already exists\")', metadata={'source': 'test_repo/template.py', 'language': 'python', 'start_index': 472}),\n",
       " Document(page_content=\"from src.helper import load_data, text_split, download_hugging_face_embeddings\\nfrom pinecone import Pinecone, ServerlessSpec\\nfrom langchain_pinecone import PineconeVectorStore\\nfrom dotenv import load_dotenv\\nimport os\\n\\nload_dotenv()\\nPINECONE_API_KEY = os.environ.get('PINECONE_API_KEY')\\nPINECONE_ENVIRONMENT = os.environ.get('PINECONE_ENVIRONMENT')\\nos.environ['PINECONE_API_KEY'] = PINECONE_API_KEY\", metadata={'source': 'test_repo/store_index.py', 'language': 'python', 'start_index': 0}),\n",
       " Document(page_content='extracted_data = load_data(data=\"data/\")\\ntext_chunks = text_split(extracted_data)\\nembeddings = download_hugging_face_embeddings()\\n\\npc = Pinecone(api_key=PINECONE_API_KEY, environment=PINECONE_ENVIRONMENT)\\n\\nindex_name = \"medical-chatbot\"', metadata={'source': 'test_repo/store_index.py', 'language': 'python', 'start_index': 399}),\n",
       " Document(page_content='pc.create_index(\\n    name=index_name,\\n    dimension=384, # Replace with your model dimensions\\n    metric=\"cosine\", # Replace with your model metric\\n    spec=ServerlessSpec(\\n        cloud=\"aws\",\\n        region=\"us-east-1\"\\n    ) \\n)\\n# embed each chunk and upsert the embeddings into Pinecone index\\ndocsearch = PineconeVectorStore.from_documents(\\n    documents=text_chunks,\\n    index_name=index_name,\\n    embedding=embeddings,\\n)', metadata={'source': 'test_repo/store_index.py', 'language': 'python', 'start_index': 637}),\n",
       " Document(page_content='from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\nfrom langchain_community.embeddings import HuggingFaceHubEmbeddings\\n\\n# Load data from PDF files\\ndef load_data(data):\\n    loader = DirectoryLoader(data,\\n                             glob=\"*.pdf\",\\n                             loader_cls=PyPDFLoader)\\n    documents = loader.load()\\n    return documents\\n\\n# split the data into text chunks', metadata={'source': 'test_repo/src/helper.py', 'language': 'python', 'start_index': 0}),\n",
       " Document(page_content='def text_split(extracted_data):\\n    text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\\n    text_chunks=text_splitter.split_documents(extracted_data)\\n    return text_chunks\\n\\n# download embeddings from haggingface\\ndef download_hugging_face_embeddings():\\n    embeddings = HuggingFaceHubEmbeddings(\\n        repo_id=\\'sentence-transformers/all-MiniLM-L6-v2\\',\\n        task=\"feature-extraction\"\\n    )\\n    return embeddings', metadata={'source': 'test_repo/src/helper.py', 'language': 'python', 'start_index': 479}),\n",
       " Document(page_content='prompt_template = \"\"\"\\nUse the following pieces of context to answer the question at the end in a detailed and informative way. \\nIf you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nProvide a comprehensive explanation that includes:\\n- Definition of the condition/term\\n- Causes and risk factors\\n- Symptoms and signs\\n- Treatment options when applicable\\n- Prevention methods when applicable\\n\\n{context}\\n\\nQuestion: {question}\\nDetailed Medical Answer:\\n\"\"\"', metadata={'source': 'test_repo/src/prompt.py', 'language': 'python', 'start_index': 0})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=OpenAIEmbeddings(disallowed_special=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(texts, embedding=embeddings, persist_directory='./db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#llm = ChatOpenAI(model_name=\"gpt-4\")\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryMemory(llm=llm, memory_key = \"chat_history\", return_message=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import search\n",
    "\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(llm, retriever=vectordb.as_retriever(search_type=\"mmr\", search_kwargs={\"k\":8}), memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what is download_hugging_face_embeddings function?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 20 is greater than number of elements in index 14, updating n_results = 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The `download_hugging_face_embeddings` function is a function that is used to download embeddings (representations of text data in a numerical format) from the Hugging Face model hub. In this specific case, the function is downloading embeddings from the \"sentence-transformers/all-MiniLM-L6-v2\" repository for the task of feature extraction. These embeddings are then used for tasks such as text similarity, clustering, and other natural language processing applications within the codebase where this function is defined.\n"
     ]
    }
   ],
   "source": [
    "from unittest import result\n",
    "\n",
    "\n",
    "result = qa(question)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmapp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
